
# 建模与调参

 - 用特征工程输出的数据直接放到模型里面来训练模型，训练结束后，在用模型来预测结果并用相应的指标来评价模型。
 - 学习调整模型参考来加快训练和获取更好的结果。

## 学习目标

 - 熟悉在金融分控领域常用的基本机器学习模型（主要介绍逻辑回归决策树）
 - 熟悉在金融分控领域常用的高阶机器学习集成模型（LightGBM、Xgboost和Random Fores）
 - 学习机器学习模型的建模过程与调参流程
 - 熟练模型性能评估的各个指标

## 基本模型：
最基本的模型包括**逻辑回归**和**决策树**模型：
**逻辑回归模型**可是说是机器学习中最简单的模型之一，但同时它具有极强的解释性，广泛的应用于各个领域，同时也可以作为很多分类算法的基础组件。例如使用GBDT + LR用于信用卡交易的反欺诈等。另外，在很多实际用用中，由于逻辑回归易于实现，可以作为很多问题baseline。
逻辑回归虽然名字中带有‘回归’，但是他是一个实实在在的分类算法，主要用于二分类。之所以叫回归，是因为该算法利用了Logistic函数（又称为sigmoid函数），将预测值映射到（0，1）之间的区间。函数形式为：</Br>
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019215553239.png#pic_center)

对应的函数图像如下：</Br>
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019215603758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkZ2hqZ2Y=,size_16,color_FFFFFF,t_70#pic_center)

**决策树分类算法**的实现关键在于如何根据训练数据构建一颗决策树。而构建决策树的核心问题就在于如何选择一个合适的分裂属性进行一次分裂以及如何制定合适的分裂标准来产生相应的分支。
在实际操作中该如何选择分裂标准——**信息熵**（Information Entropy)的概念。算法会根据所有样本 **信息熵的变化（信息增益）**来选择最佳分类。 因而信息熵就是决策树方法中分支产生的衡量标准之一。

**信息熵**来源热力学中的**熵（Entropy）**的概念。最早由德国物理学家克劳修斯提出，用来表述某个体系的混乱程度。在此基础上，美国的数学家，信息论的创始人香农提出了信息熵的概念。它代表了一个给定数据集中的不确定性或者随机性程度的度量。当一个数据集中的记录全部为同一类时，则没有不确定性，此时熵为0。因此我们也可以把熵看成是信息的数学期望。若要求出熵的大小，则先要计算信息：

设某个事物具有种相互独立的可能结果，则信息的定义为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220143140.png#pic_center)

为什么是以2为底的对数？这个可能和信息中的最小单位比特有关（一个比特只有2种状态），其中![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220328205.png#pic_center)

表示第![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220351983.png#pic_center)
个分类，

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220402336.png#pic_center)
表示第个分类的概率函数，并且：![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220437267.png#pic_center)

由此，信息熵![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220459450.png#pic_center)
就可以表示为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220529552.png#pic_center)

对于一个简单的二元分类，此时![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220617885.png#pic_center)
，那么其熵为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201019220627180.png#pic_center)

通过信息熵我们可以精确的度量信息量的大小，香农也因为这一成就成为信息学领域公认的奠基人。我觉得这一成就完全可以媲美牛顿的万有引力定律和爱因斯坦的质能方程，前者确统一了宇宙所有天体的运行规律，后者深刻揭示了质量和能量只不过是同一事物不同的侧面




他们的优缺点可以用下面这张表格来概括：
![image.png](https://img-blog.csdnimg.cn/20201019214607471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FkZ2hqZ2Y=,size_16,color_FFFFFF,t_70#pic_center)

## 集成模型集成方法（ensemble method）
针对决策树的缺点，大神们发明了各种集成方法。即在单颗决策树的基础上通过加入集成算法的思想，形成了很多集成模型，即将多个弱学习器组合成一个强分类器，因此集成学习的泛化能力一般比单一分类器要好，以此来解决单颗决策树过拟合的问题。

集成方法主要包括Bagging和Boosting，Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个更加强大的分类。两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果。常见的基于Baggin思想的集成模型有：随机森林、基于Boosting思想的集成模型有：Adaboost、GBDT、XgBoost、LightGBM等。

Baggin和Boosting的区别总结如下：

样本选择上： Bagging方法的训练集是从原始集中有放回的选取，所以从原始集中选出的各轮训练集之间是独立的；而Boosting方法需要每一轮的训练集不变，只是训练集中每个样本在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整
样例权重上： Bagging方法使用均匀取样，所以每个样本的权重相等；而Boosting方法根据错误率不断调整样本的权值，错误率越大则权重越大
预测函数上： Bagging方法中所有预测函数的权重相等；而Boosting方法中每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重
并行计算上： Bagging方法中各个预测函数可以并行生成；而Boosting方法各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

## 模型评估方法
对于模型来说，其在训练集上面的误差我们称之为训练误差或者经验误差，而在测试集上的误差称之为测试误差。

对于我们来说，我们更关心的是模型对于新样本的学习能力，即我们希望通过对已有样本的学习，尽可能的将所有潜在样本的普遍规律学到手，而如果模型对训练样本学的太好，则有可能把训练样本自身所具有的一些特点当做所有潜在样本的普遍特点，这时候我们就会出现过拟合的问题。

因此我们通常将已有的数据集划分为训练集和测试集两部分，其中训练集用来训练模型，而测试集则是用来评估模型对于新样本的判别能力。

对于数据集的划分，我们通常要保证满足以下两个条件：

训练集和测试集的分布要与样本真实分布一致，即训练集和测试集都要保证是从样本真实分布中独立同分布采样而得；
训练集和测试集要互斥

## 模型调参

 - 贪心调参
先使用当前对模型影响最大的参数进行调优，达到当前参数下的模型最优化，再使用对模型影响次之的参数进行调优，如此下去，直到所有的参数调整完毕。

这个方法的缺点就是可能会调到局部最优而不是全局最优，但是只需要一步一步的进行参数最优化调试即可，容易理解。

需要注意的是在树模型中参数调整的顺序，也就是各个参数对模型的影响程度，这里列举一下日常调参过程中常用的参数和调参顺序：

①：max_depth、num_leaves
②：min_data_in_leaf、min_child_weight
③：bagging_fraction、 feature_fraction、bagging_freq
④：reg_lambda、reg_alpha
⑤：min_split_gain

 - 网格搜索
sklearn 提供GridSearchCV用于进行网格搜索，只需要把模型的参数输进去，就能给出最优化的结果和参数。相比起贪心调参，网格搜索的结果会更优，但是网格搜索只适合于小数据集，一旦数据的量级上去了，很难得出结果。

 - 贝叶斯调参
贝叶斯调参的主要思想是：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布）。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。

贝叶斯调参的步骤如下：

定义优化函数(rf_cv）
建立模型
定义待优化的参数
得到优化结果，并返回要优化的分数指标


